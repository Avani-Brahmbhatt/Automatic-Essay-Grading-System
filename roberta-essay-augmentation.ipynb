{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:19:36.836825Z","iopub.status.busy":"2025-03-26T05:19:36.836519Z","iopub.status.idle":"2025-03-26T05:19:41.690176Z","shell.execute_reply":"2025-03-26T05:19:41.689247Z","shell.execute_reply.started":"2025-03-26T05:19:36.836801Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nlpaug\n","  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (2.32.3)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (5.2.0)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n","Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install nlpaug numpy pandas tqdm torch transformers sentencepiece gensim"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:19:46.710562Z","iopub.status.busy":"2025-03-26T05:19:46.710231Z","iopub.status.idle":"2025-03-26T05:20:22.701628Z","shell.execute_reply":"2025-03-26T05:20:22.700963Z","shell.execute_reply.started":"2025-03-26T05:19:46.710535Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","from tqdm import tqdm\n","import nlpaug.augmenter.word as naw\n","from nlpaug.util.file.download import DownloadUtil\n","import torch"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:20:24.499516Z","iopub.status.busy":"2025-03-26T05:20:24.498849Z","iopub.status.idle":"2025-03-26T05:20:25.266096Z","shell.execute_reply":"2025-03-26T05:20:25.265293Z","shell.execute_reply.started":"2025-03-26T05:20:24.499485Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Full dataset size: 17645\n","Original score distribution:\n","score\n","1    1252\n","2    4723\n","3    6280\n","4    3926\n","5     970\n","6     494\n","Name: count, dtype: int64\n"]}],"source":["# Load dataset\n","file_path = '/kaggle/input/17k-essays/essay_training_set.csv' #give path to 17kEssays file\n","df = pd.read_csv(file_path).dropna()\n","\n","print(f\"Full dataset size: {len(df)}\")\n","print(\"Original score distribution:\")\n","print(df['score'].value_counts().sort_index())"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:20:34.100957Z","iopub.status.busy":"2025-03-26T05:20:34.100599Z","iopub.status.idle":"2025-03-26T05:20:34.114434Z","shell.execute_reply":"2025-03-26T05:20:34.113526Z","shell.execute_reply.started":"2025-03-26T05:20:34.100931Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training set size: 14116\n","Training score distribution:\n","score\n","1    1002\n","2    3758\n","3    5040\n","4    3123\n","5     781\n","6     412\n","Name: count, dtype: int64\n","Validation set size: 3529\n","Validation score distribution:\n","score\n","1     250\n","2     965\n","3    1240\n","4     803\n","5     189\n","6      82\n","Name: count, dtype: int64\n"]}],"source":["# Split into train and validation sets (80% train, 20% validation)\n","train_df = df.sample(frac=0.8, random_state=42)\n","val_df = df.drop(train_df.index)\n","\n","print(f\"\\nTraining set size: {len(train_df)}\")\n","print(\"Training score distribution:\")\n","print(train_df['score'].value_counts().sort_index())\n","print(f\"Validation set size: {len(val_df)}\")\n","print(\"Validation score distribution:\")\n","print(val_df['score'].value_counts().sort_index())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:20:43.787971Z","iopub.status.busy":"2025-03-26T05:20:43.787578Z","iopub.status.idle":"2025-03-26T05:20:48.115109Z","shell.execute_reply":"2025-03-26T05:20:48.114123Z","shell.execute_reply.started":"2025-03-26T05:20:43.787944Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ee1b61b34b44a21bb0e78d8728d2235","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5162983486d7485c94075616d606bd5d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dccc03211c224c299d8ac8e78ccedc60","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7010dbdbb50c44a4b30e1ddd1c4a3b9b","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4cc483035e04c7484ecac4b618b6981","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b099a2acd66d41a18629cb9e895a1c38","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# RoBERTa-based augmenter (contextual embeddings)\n","roberta_aug = naw.ContextualWordEmbsAug(\n","    model_path='roberta-base', \n","    action=\"substitute\",  # or \"insert\"\n","    aug_p=0.3, \n","    aug_min=1, \n","    aug_max=5,\n","    device='cuda' if torch.cuda.is_available() else 'cpu'\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:20:54.335379Z","iopub.status.busy":"2025-03-26T05:20:54.335033Z","iopub.status.idle":"2025-03-26T05:20:55.070436Z","shell.execute_reply":"2025-03-26T05:20:55.069550Z","shell.execute_reply.started":"2025-03-26T05:20:54.335354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Original: Data augmentation is important for NLP tasks.\n","Augmented: ['Data sources are important across NLP tasks.']\n"]}],"source":["# Example text\n","text = \"Data augmentation is important for NLP tasks.\"\n","augmented_text = roberta_aug.augment(text)\n","\n","print(f\"Original: {text}\")\n","print(f\"Augmented: {augmented_text}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:21:15.603801Z","iopub.status.busy":"2025-03-26T05:21:15.603483Z","iopub.status.idle":"2025-03-26T05:47:38.807791Z","shell.execute_reply":"2025-03-26T05:47:38.806899Z","shell.execute_reply.started":"2025-03-26T05:21:15.603776Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Augmenting minority score 1 from 1002 to ~3990\n","Each essay augmented 3 times\n"]},{"name":"stderr","output_type":"stream","text":["Score 1 Progress: 100%|██████████| 3006/3006 [05:07<00:00,  9.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Augmenting minority score 5 from 781 to ~3990\n","Each essay augmented 5 times\n"]},{"name":"stderr","output_type":"stream","text":["Score 5 Progress: 100%|██████████| 3905/3905 [12:04<00:00,  5.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Augmenting minority score 6 from 412 to ~3990\n","Each essay augmented 9 times\n"]},{"name":"stderr","output_type":"stream","text":["Score 6 Progress: 100%|██████████| 3708/3708 [09:11<00:00,  6.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Adjusting majority score 2 from 3758 to ~3990\n","Each essay augmented 1 times\n"]},{"name":"stderr","output_type":"stream","text":["Score 2 Progress: 100%|██████████| 3758/3758 [00:00<00:00, 21498.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Downsampling majority score 3 from 5040 to 3990\n"]},{"name":"stderr","output_type":"stream","text":["Score 3 Progress: 100%|██████████| 3990/3990 [00:00<00:00, 21339.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Adjusting majority score 4 from 3123 to ~3990\n","Each essay augmented 1 times\n"]},{"name":"stderr","output_type":"stream","text":["Score 4 Progress: 100%|██████████| 3123/3123 [00:00<00:00, 19310.47it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Original training set size: 14116\n","Augmented training set size: 21490\n","Validation set size (unchanged): 3529\n","Final combined dataset size: 25019\n","Final score distribution:\n","score\n","1    3256\n","2    4723\n","3    5230\n","4    3926\n","5    4094\n","6    3790\n","Name: count, dtype: int64\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["#Augmentation logic (only for training set)\n","target_count = 3990\n","minority_scores = [1, 5, 6]\n","majority_scores = [2, 3, 4]\n","\n","augmented_texts = []\n","augmented_scores = []\n","\n","augmenter = roberta_aug  # For RoBERTa\n","\n","# Augment minority classes in training set\n","for score in minority_scores:\n","    minority_df = train_df[train_df['score'] == score]\n","    original_count = len(minority_df)\n","    augmentation_factor = max(1, min(int(target_count / original_count), 12))\n","    \n","    print(f\"\\nAugmenting minority score {score} from {original_count} to ~{target_count}\")\n","    print(f\"Each essay augmented {augmentation_factor} times\")\n","    \n","    with tqdm(total=original_count * augmentation_factor, desc=f\"Score {score} Progress\") as pbar:\n","        for _, row in minority_df.iterrows():\n","            augmented_texts.append(row['full_text'])  # Original\n","            augmented_scores.append(row['score'])\n","            pbar.update(1)\n","            for _ in range(augmentation_factor - 1):\n","                new_text = augmenter.augment(row['full_text'])[0]  # nlpaug returns a list\n","                augmented_texts.append(new_text)\n","                augmented_scores.append(row['score'])\n","                pbar.update(1)\n","\n","# Adjust majority classes in training set\n","for score in majority_scores:\n","    majority_df = train_df[train_df['score'] == score]\n","    original_count = len(majority_df)\n","    \n","    if original_count > target_count:\n","        sampled_df = majority_df.sample(n=target_count, random_state=42)\n","        print(f\"\\nDownsampling majority score {score} from {original_count} to {target_count}\")\n","        adjustment_factor = 1\n","    else:\n","        adjustment_factor = max(1, min(int(target_count / original_count), 2))\n","        sampled_df = majority_df\n","        print(f\"\\nAdjusting majority score {score} from {original_count} to ~{target_count}\")\n","        print(f\"Each essay augmented {adjustment_factor} times\")\n","    \n","    with tqdm(total=len(sampled_df) * adjustment_factor, desc=f\"Score {score} Progress\") as pbar:\n","        for _, row in sampled_df.iterrows():\n","            augmented_texts.append(row['full_text'])  # Original\n","            augmented_scores.append(row['score'])\n","            pbar.update(1)\n","            for _ in range(adjustment_factor - 1):\n","                new_text = augmenter.augment(row['full_text'])[0]\n","                augmented_texts.append(new_text)\n","                augmented_scores.append(row['score'])\n","                pbar.update(1)\n","\n","# Create augmented training DataFrame\n","augmented_train_df = pd.DataFrame({'full_text': augmented_texts, 'score': augmented_scores})\n","\n","# Combine with validation set (unaugmented)\n","final_df = pd.concat([augmented_train_df, val_df], ignore_index=True)\n","\n","print(f\"\\nOriginal training set size: {len(train_df)}\")\n","print(f\"Augmented training set size: {len(augmented_train_df)}\")\n","print(f\"Validation set size (unchanged): {len(val_df)}\")\n","print(f\"Final combined dataset size: {len(final_df)}\")\n","print(\"Final score distribution:\")\n","print(final_df['score'].value_counts().sort_index())"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-03-26T05:51:28.541090Z","iopub.status.busy":"2025-03-26T05:51:28.540800Z","iopub.status.idle":"2025-03-26T05:51:30.790170Z","shell.execute_reply":"2025-03-26T05:51:30.789250Z","shell.execute_reply.started":"2025-03-26T05:51:28.541069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Datasets saved!\n"]}],"source":["augmented_train_df.to_csv('augmented_train_dataset.csv', index=False)\n","val_df.to_csv('validation_dataset.csv', index=False)\n","final_df.to_csv('complete_essay_dataset.csv', index=False)\n","print(\"Datasets saved!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6836113,"sourceId":10984033,"sourceType":"datasetVersion"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
